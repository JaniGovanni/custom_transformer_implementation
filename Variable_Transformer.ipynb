{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u9J_5U3RttI"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import time\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23DJQTBIeLT7"
      },
      "source": [
        "# Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE-xhTWGSFbX"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class MultiHeadAttentionEinsum(layers.Layer):\n",
        "  def __init__(self, d_k, d_model, n_heads, cm_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_k = d_k\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    self.key = layers.Dense(units=self.d_k * n_heads)\n",
        "    self.query = layers.Dense(units=self.d_k * n_heads)\n",
        "    self.value = layers.Dense(units=self.d_k * n_heads)\n",
        "\n",
        "    # search for better way to handle this\n",
        "    if cm_dim is not None:\n",
        "      cm = tf.experimental.numpy.tril(np.ones((cm_dim, cm_dim)), k=0)\n",
        "      self.cm = tf.reshape(cm, (1, 1, cm_dim, cm_dim))\n",
        "    else:\n",
        "      self.cm = None\n",
        "\n",
        "    # out projection\n",
        "    self.fc = layers.Dense(units=d_model)\n",
        "\n",
        "  def call(self, q, k, v, mask):\n",
        "\n",
        "    queries = self.query(q)\n",
        "    keys = self.key(k)\n",
        "    values = self.value(v)\n",
        "\n",
        "\n",
        "    N = tf.shape(queries)[0]\n",
        "    T = tf.shape(queries)[1]\n",
        "\n",
        "    queries = tf.reshape(queries,(N, T, self.n_heads, self.d_k))\n",
        "    keys = tf.reshape(keys,(N, T, self.n_heads, self.d_k))\n",
        "    values = tf.reshape(values,(N, T, self.n_heads, self.d_k))\n",
        "\n",
        "    attn_scores = tf.einsum('nthd,nshd,ns->nhts', queries,\n",
        "                            keys,\n",
        "                            tf.cast(mask, tf.float32)) / math.sqrt(self.d_k)\n",
        "    if self.cm is not None:\n",
        "      attn_scores = tf.where(self.cm[:, :, :T, :T] == 0,\n",
        "                             0,\n",
        "                             attn_scores)\n",
        "\n",
        "    attn_weights = tf.nn.softmax(tf.where(attn_scores == 0, float('-inf'), attn_scores),\n",
        "                                 axis=-1)\n",
        "\n",
        "    A = tf.einsum('bits,bshd->bthd', attn_weights, values)\n",
        "    A = tf.reshape(A, (N, T, self.n_heads * self.d_k))\n",
        "    return self.fc(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE1iiS5SaQ0S"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class PositionalEncoding(layers.Layer):\n",
        "  def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
        "    super().__init__()\n",
        "    self.dropout = layers.Dropout(rate=dropout_prob)\n",
        "\n",
        "    position = tf.range(max_len, dtype=tf.float32)[:, None]\n",
        "\n",
        "    # 1/10000^(2i/d_model)\n",
        "    exp_term = tf.range(d_model, delta=2,  dtype=tf.float32)\n",
        "    div_term = tf.exp(exp_term * (-math.log(10000.0) / d_model))\n",
        "\n",
        "    # pe is of shape (1,T,d_model)\n",
        "    pe = np.zeros((1, max_len, d_model))\n",
        "\n",
        "    # multiplication instead of difidation because a - sign\n",
        "    # was added in the exponent of the div term\n",
        "    # select the even indices for sin and odd for cos\n",
        "    pe[0, :, 0::2] = tf.sin(position * div_term)\n",
        "    pe[0, :, 1::2] = tf.cos(position * div_term)\n",
        "    self.pe = tf.cast(pe, dtype=tf.float32)\n",
        "\n",
        "  def call(self, x):\n",
        "    # x: (N, T, d_model)\n",
        "    x = x + self.pe[:, :tf.shape(x)[1], :]\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR8sNZFghlfi"
      },
      "source": [
        "pos = 0...T-1 (one for every position in the sequence)\n",
        "\n",
        "i = 0...d_model-1 (one for every dimension)\n",
        "\n",
        "$\n",
        "pe(pos, 2i) = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n",
        "$\n",
        "\n",
        "$\n",
        "pe(pos, 2i+1) = cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bbiQhTlhrb9"
      },
      "source": [
        "Implementation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FspUh8XNhmpA"
      },
      "source": [
        "$\n",
        "e^{\\frac{2i * (-log1000)}{d_{model}}}\n",
        "$ = $\n",
        "(e^{-log1000})^{\\frac{2i}{d_{model}}}\n",
        "$ = $\n",
        "10000^{\\frac{-2i}{d_{model}}}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSI8Z1NaaTbf"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(self, d_k, d_model, n_heads, dropout_prob, cm_dim):\n",
        "    super().__init__()\n",
        "    self.ln1 = layers.LayerNormalization()\n",
        "    self.ln2 = layers.LayerNormalization()\n",
        "\n",
        "    self.mha = MultiHeadAttentionEinsum(d_k,\n",
        "                                        d_model,\n",
        "                                        n_heads,\n",
        "                                        cm_dim)\n",
        "    self.ann = tf.keras.Sequential([\n",
        "        layers.Dense(units=d_k * 4, activation=tf.nn.gelu),\n",
        "        layers.Dense(units=d_model),\n",
        "        layers.Dropout(rate=dropout_prob)]\n",
        "    )\n",
        "    self.dropout = layers.Dropout(rate=dropout_prob)\n",
        "\n",
        "  def call(self, x, mask):\n",
        "    # residual connections\n",
        "    x = self.ln1(x + self.mha(x, x, x, mask))\n",
        "    x = self.ln2(x + self.ann(x))\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnUkGc7lTB6c"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TransformerConfig:\n",
        "  d_k: int = 16\n",
        "  d_model: int = 64\n",
        "  n_heads: int = 4\n",
        "  n_layers: int = 2\n",
        "  dropout_prob: float = 0.1\n",
        "  n_classes: int = None\n",
        "  vocab_size: int = None\n",
        "  max_len: int = None\n",
        "\n",
        "  def __post_init__(self):\n",
        "    if self.vocab_size == None:\n",
        "      raise ValueError(\"vocab size cannot be none\")\n",
        "    if self.max_len == None:\n",
        "      raise ValueError(f'max len cannot be none')\n",
        "    if self.n_classes == None:\n",
        "      print(\"n_classes is none, using language model head.\")\n",
        "      self.n_classes = self.vocab_size\n",
        "    if self.max_len == None:\n",
        "      raise ValueError(f'max_len cannot be none')\n",
        "    self.decoder = self.n_classes == self.vocab_size\n",
        "    self.cm_dim = self.max_len if self.decoder else None\n",
        "\n",
        "  def create_model(self, metrics=['accuracy'], optimizer=None, loss_fn=None):\n",
        "    \"\"\"\n",
        "    Creates an EncoderOrDecoder Layer from the config parameters and wraps it into a Tensorflow model.\n",
        "    \"\"\"\n",
        "    layer = EncoderOrDecoder(self.d_k,\n",
        "                             self.d_model,\n",
        "                             self.n_heads,\n",
        "                             self.n_layers,\n",
        "                             self.dropout_prob,\n",
        "                             self.n_classes,\n",
        "                             self.vocab_size,\n",
        "                             self.max_len,\n",
        "                             self.decoder,\n",
        "                             self.cm_dim)\n",
        "    if optimizer is None:\n",
        "      optimizer = keras.optimizers.AdamW()\n",
        "    if loss_fn is None:\n",
        "      loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "    inputs = {\n",
        "        'input_ids': layers.Input(shape=(None,), name='input_ids', dtype=tf.int32),\n",
        "        'attention_mask': layers.Input(shape=(None,), name='attention_mask', dtype=tf.int32)\n",
        "    }\n",
        "\n",
        "    outputs = layer(inputs['input_ids'], inputs['attention_mask'])\n",
        "\n",
        "    model = keras.Model(inputs=inputs,\n",
        "                        outputs=outputs)\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "    return model\n",
        "\n",
        "    def create_layer(self):\n",
        "      \"\"\"\n",
        "      Creates an EncoderOrDecoder Layer from the config parameters.\n",
        "      \"\"\"\n",
        "\n",
        "    return EncoderOrDecoder(self.d_k,\n",
        "                            self.d_model,\n",
        "                            self.n_heads,\n",
        "                            self.n_layers,\n",
        "                            self.dropout_prob,\n",
        "                            self.n_classes,\n",
        "                            self.vocab_size,\n",
        "                            self.max_len,\n",
        "                            self.decoder,\n",
        "                            self.cm_dim)\n",
        "\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class EncoderOrDecoder(layers.Layer):\n",
        "  def __init__(self,\n",
        "               d_k: int,\n",
        "               d_model: int,\n",
        "               n_heads: int,\n",
        "               n_layers: int,\n",
        "               dropout_prob: float,\n",
        "               n_classes: int,\n",
        "               vocab_size: int,\n",
        "               max_len: int,\n",
        "               decoder: bool,\n",
        "               cm_dim,\n",
        "               **kwargs):\n",
        "    super().__init__()\n",
        "    self.decoder = decoder\n",
        "    self.embedding = layers.Embedding(input_dim=vocab_size,\n",
        "                                      output_dim=d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model,\n",
        "                                           max_len,\n",
        "                                           dropout_prob)\n",
        "\n",
        "    self.transformer_blocks = [\n",
        "        TransformerBlock(\n",
        "            d_k,\n",
        "            d_model,\n",
        "            n_heads,\n",
        "            dropout_prob,\n",
        "            cm_dim) for _ in range(n_layers)]\n",
        "    self.ln = layers.LayerNormalization()\n",
        "    self.fc = layers.Dense(units=n_classes)\n",
        "\n",
        "  def call(self, x, mask):\n",
        "    x = self.embedding(x)\n",
        "    x = self.pos_encoding(x)\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(x, mask)\n",
        "\n",
        "    if not self.decoder:\n",
        "      x = x[:, 0, :]\n",
        "\n",
        "    x = self.ln(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class keras_text_class_pipeline:\n",
        "  def __init__(self,model_path, tokenizer):\n",
        "    self.model = tf.keras.models.load_model(model_path)\n",
        "    if tokenizer is None:\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
        "    else:\n",
        "      self.tokenizer = tokenizer\n",
        "  def __call__(self, input_sentence):\n",
        "    input = self.tokenizer(input_sentence,\n",
        "                           return_tensors='tf',\n",
        "                           truncation=True,\n",
        "                           return_attention_mask=True)\n",
        "    return self.model({'input_ids': input['input_ids'],\n",
        "                       'attention_mask': input['attention_mask']})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sca4eEjDKg1a"
      },
      "source": [
        "## Deprecated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TdaA2A-GZzF"
      },
      "outputs": [],
      "source": [
        "# deprecated, use built in methods\n",
        "import time\n",
        "def train_model(model,\n",
        "                  tf_train_set,\n",
        "                  tf_eval_set,\n",
        "                  loss_fn,\n",
        "                  optimizer,\n",
        "                  epochs=2):\n",
        "  acc_metric = {\n",
        "      'train': keras.metrics.SparseCategoricalAccuracy(),\n",
        "      'val': keras.metrics.SparseCategoricalAccuracy()\n",
        "  }\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "      start_time = time.time()\n",
        "\n",
        "\n",
        "      for step, (inputs, labels) in enumerate(tf_train_set):\n",
        "          with tf.GradientTape() as tape:\n",
        "              logits = model(inputs)\n",
        "              loss_value = loss_fn(labels, logits)\n",
        "          grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "          optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "          acc_metric['train'].update_state(labels, logits)\n",
        "          # Log every 200 batches.\n",
        "          if step % 200 == 0:\n",
        "              print(\n",
        "                  \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                  % (step, float(loss_value))\n",
        "              )\n",
        "\n",
        "      # Display metrics at the end of each epoch.\n",
        "      train_acc = acc_metric['train'].result()\n",
        "      print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "      # Reset training metrics at the end of each epoch\n",
        "      acc_metric['train'].reset_state()\n",
        "\n",
        "      # Run a validation loop at the end of each epoch.\n",
        "      for inputs, labels_val in tf_eval_set:\n",
        "        val_logits = model.predict(inputs, verbose=0)\n",
        "        val_logits = val_logits[:labels_val.shape[0]]\n",
        "        acc_metric['val'].update_state(labels_val, val_logits)\n",
        "\n",
        "      print(\"Running Validation...\")\n",
        "      val_acc = acc_metric['val'].result()\n",
        "\n",
        "      print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "\n",
        "      acc_metric['val'].reset_state()\n",
        "      print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_layer(layer,\n",
        "                tf_train_set,\n",
        "                tf_eval_set,\n",
        "                loss_fn,\n",
        "                optimizer,\n",
        "                epochs=2):\n",
        "  acc_metric = {\n",
        "      'train': keras.metrics.SparseCategoricalAccuracy(),\n",
        "      'val': keras.metrics.SparseCategoricalAccuracy()\n",
        "  }\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "      start_time = time.time()\n",
        "\n",
        "\n",
        "      for step, (inputs, labels) in enumerate(tf_train_set):\n",
        "          with tf.GradientTape() as tape:\n",
        "              logits = layer(inputs['input_ids'],\n",
        "                             inputs['attention_mask'],\n",
        "                             training=True)\n",
        "              loss_value = loss_fn(labels, logits)\n",
        "          grads = tape.gradient(loss_value, layer.trainable_weights)\n",
        "          optimizer.apply_gradients(zip(grads, layer.trainable_weights))\n",
        "\n",
        "          acc_metric['train'].update_state(labels, logits)\n",
        "          # Log every 200 batches.\n",
        "          if step % 200 == 0:\n",
        "              print(\n",
        "                  \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                  % (step, float(loss_value))\n",
        "              )\n",
        "\n",
        "      # Display metrics at the end of each epoch.\n",
        "      train_acc = acc_metric['train'].result()\n",
        "      print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "      # Reset training metrics at the end of each epoch\n",
        "      acc_metric['train'].reset_state()\n",
        "\n",
        "      # Run a validation loop at the end of each epoch.\n",
        "      for inputs, labels_val in tf_eval_set:\n",
        "        val_logits = layer(inputs['input_ids'],\n",
        "                           inputs['attention_mask'],\n",
        "                           training=False)\n",
        "        val_logits = val_logits[:labels_val.shape[0]]\n",
        "        acc_metric['val'].update_state(labels_val, val_logits)\n",
        "\n",
        "      print(\"Running Validation...\")\n",
        "      val_acc = acc_metric['val'].result()\n",
        "\n",
        "      print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "\n",
        "      acc_metric['val'].reset_state()\n",
        "      print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "  return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_1AmgQAy1Tb"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAKDgHBQy59o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/IMDB-Dataset.csv')\n",
        "\n",
        "dataset = Dataset.from_pandas(data)\n",
        "dataset = dataset.train_test_split(test_size=0.3)\n",
        "\n",
        "label2id = {'negative': 0, 'positive': 1}\n",
        "id2label = {0:'negative', 1:'positive'}\n",
        "\n",
        "dataset = dataset.map(lambda x: {'labels': label2id[x['sentiment']]})\n",
        "\n",
        "checkpoint = 'distilbert-base-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "  return tokenizer(batch['review'], truncation=True)\n",
        "tokenized_datasets = dataset.map(tokenize_fn,\n",
        "                                 batched=True)\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentiment\", \"review\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC80-boX0c71"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer,\n",
        "                                        return_tensors=\"tf\")\n",
        "\n",
        "tf_train_set = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator)\n",
        "\n",
        "# evaluation dataset is mislabeld\n",
        "tf_eval_set = tokenized_datasets[\"test\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.AdamW()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "config = TransformerConfig(vocab_size=tokenizer.vocab_size,\n",
        "                           max_len=tokenizer.model_max_length,\n",
        "                           d_k=8,\n",
        "                           d_model=16,\n",
        "                           n_heads=1,\n",
        "                           n_layers=1,\n",
        "                           n_classes=len(set(tokenized_datasets['train']['labels'])),\n",
        "                           dropout_prob=0.2)\n",
        "\n",
        "encoder_model = config.create_model(metrics=['accuracy'],\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKiL197Y1yy5"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"trained_model.keras\"\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                         save_weights_only=False,\n",
        "                                                         monitor=\"val_accuracy\",\n",
        "                                                         save_best_only=True)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                 factor=0.2,\n",
        "                                                 patience=1,\n",
        "                                                 verbose=1, # print out when learning rate goes down\n",
        "                                                 min_lr=1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMIpO1vOHOve",
        "outputId": "7220e331-5d9d-4114-9bdb-8aa5cd0a327f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 280ms/step - accuracy: 0.5508 - loss: 0.6736 - val_accuracy: 0.8715 - val_loss: 0.3059 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 120ms/step - accuracy: 0.8737 - loss: 0.3117 - val_accuracy: 0.8937 - val_loss: 0.2609 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9086 - loss: 0.2366\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 47ms/step - accuracy: 0.9086 - loss: 0.2366 - val_accuracy: 0.8903 - val_loss: 0.2805 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m2183/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9340 - loss: 0.1834\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 69ms/step - accuracy: 0.9340 - loss: 0.1834 - val_accuracy: 0.8981 - val_loss: 0.2787 - learning_rate: 2.0000e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m2187/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9350 - loss: 0.1787\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 37ms/step - accuracy: 0.9350 - loss: 0.1787 - val_accuracy: 0.8981 - val_loss: 0.2832 - learning_rate: 4.0000e-05\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c6eb12a61a0>"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder_model.fit(tf_train_set,\n",
        "                  epochs=5,\n",
        "                  validation_data=tf_eval_set,\n",
        "                  callbacks=[checkpoint_callback, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-i-ag5KjDxa",
        "outputId": "1ff438ea-8904-4839-e101-952780eb1233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.8991 - loss: 0.2796\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.28324806690216064, 0.8981333374977112]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder_model.evaluate(tf_eval_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViMYCoXYDGQF"
      },
      "outputs": [],
      "source": [
        "encoder_model.save('trained_model.keras')\n",
        "loaded_model = tf.keras.models.load_model('trained_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTHmlhUGjNf2",
        "outputId": "5fd0d52b-f974-406c-b1df-961f60ed65b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 74ms/step - accuracy: 0.8991 - loss: 0.2796\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.28324806690216064, 0.8981333374977112]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loaded_model.evaluate(tf_eval_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo4m3vgFwg1q"
      },
      "outputs": [],
      "source": [
        "pipe = keras_text_class_pipeline('trained_model.keras', tokenizer)\n",
        "pipe(['this movie was great'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nJP4OYmW4xW_",
        "outputId": "81bb6c1c-ea33-4062-ee5e-a8e48d860268"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_d3c69b4d-26be-402f-9231-69f3268b7700\", \"trained_model.keras\", 5661111)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('trained_model.keras')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Sca4eEjDKg1a",
        "PxN2u1oZeP8i",
        "N_1AmgQAy1Tb"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
